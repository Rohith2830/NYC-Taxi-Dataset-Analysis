{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmVm8LmjC4wzt2lPf2xzDn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohith2830/NYC-Taxi-Dataset-Analysis/blob/main/NYC_Taxi_Dataset_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq"
      ],
      "metadata": {
        "id": "k8JtfHTRZfkw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://downloads.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdBYOQBBZlFT",
        "outputId": "42e312ef-b28c-486f-eb72-91c49fd75262"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-11 12:12:17--  https://downloads.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 135.181.214.104, 88.99.208.237, 2a01:4f9:3a:2c57::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|135.181.214.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2025-07-11 12:12:17 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: !wget https://downloads.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "\n",
        "!tar xf spark-3.3.2-bin-hadoop3.tgz\n",
        "!pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD6iu1IMZnSq",
        "outputId": "7dfd0ed8-0f31-486f-8753-3dca6a82a16f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar: spark-3.3.2-bin-hadoop3.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: findspark in /usr/local/lib/python3.11/dist-packages (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark\n"
      ],
      "metadata": {
        "id": "EIRIfK-XZyHi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6d101d1",
        "outputId": "a34374d1-fb33-4674-ffbf-58bf8127070e"
      },
      "source": [
        "# Download a valid Spark version (example using Spark 3.3.0)\n",
        "!wget https://archive.apache.org/dist/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz\n",
        "\n",
        "# Extract Spark\n",
        "!tar -xvzf spark-3.3.0-bin-hadoop3.tgz > /dev/null\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.0-bin-hadoop3\"\n",
        "\n",
        "# Install and initialize findspark\n",
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-11 12:13:57--  https://archive.apache.org/dist/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 299321244 (285M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.3.0-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.3.0-bin-had 100%[===================>] 285.45M  24.5MB/s    in 11s     \n",
            "\n",
            "2025-07-11 12:14:09 (25.0 MB/s) - ‘spark-3.3.0-bin-hadoop3.tgz’ saved [299321244/299321244]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"NYC_Taxi_Analysis\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "tqSpYVoLaLZJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"/content/yellow_tripdata_2020-01.csv\", header=True, inferSchema=True)\n",
        "df.printSchema()\n",
        "df.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5ILqnjFaMMX",
        "outputId": "533deac8-f2e3-4ba2-d60b-42740d5e948a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- VendorID: integer (nullable = true)\n",
            " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
            " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
            " |-- passenger_count: integer (nullable = true)\n",
            " |-- trip_distance: double (nullable = true)\n",
            " |-- RatecodeID: integer (nullable = true)\n",
            " |-- store_and_fwd_flag: string (nullable = true)\n",
            " |-- PULocationID: integer (nullable = true)\n",
            " |-- DOLocationID: integer (nullable = true)\n",
            " |-- payment_type: integer (nullable = true)\n",
            " |-- fare_amount: double (nullable = true)\n",
            " |-- extra: double (nullable = true)\n",
            " |-- mta_tax: double (nullable = true)\n",
            " |-- tip_amount: double (nullable = true)\n",
            " |-- tolls_amount: double (nullable = true)\n",
            " |-- improvement_surcharge: double (nullable = true)\n",
            " |-- total_amount: double (nullable = true)\n",
            " |-- congestion_surcharge: double (nullable = true)\n",
            "\n",
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
            "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
            "|       1| 2020-01-01 00:28:15|  2020-01-01 00:33:03|              1|          1.2|         1|                 N|         238|         239|           1|        6.0|  3.0|    0.5|      1.47|         0.0|                  0.3|       11.27|                 2.5|\n",
            "|       1| 2020-01-01 00:35:39|  2020-01-01 00:43:04|              1|          1.2|         1|                 N|         239|         238|           1|        7.0|  3.0|    0.5|       1.5|         0.0|                  0.3|        12.3|                 2.5|\n",
            "|       1| 2020-01-01 00:47:41|  2020-01-01 00:53:52|              1|          0.6|         1|                 N|         238|         238|           1|        6.0|  3.0|    0.5|       1.0|         0.0|                  0.3|        10.8|                 2.5|\n",
            "|       1| 2020-01-01 00:55:23|  2020-01-01 01:00:14|              1|          0.8|         1|                 N|         238|         151|           1|        5.5|  0.5|    0.5|      1.36|         0.0|                  0.3|        8.16|                 0.0|\n",
            "|       2| 2020-01-01 00:01:58|  2020-01-01 00:04:16|              1|          0.0|         1|                 N|         193|         193|           2|        3.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         4.8|                 0.0|\n",
            "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_with_revenue = df.withColumn(\"Revenue\",\n",
        "    col(\"fare_amount\") + col(\"extra\") + col(\"mta_tax\") +\n",
        "    col(\"improvement_surcharge\") + col(\"tip_amount\") +\n",
        "    col(\"tolls_amount\") + col(\"total_amount\")\n",
        ")\n",
        "\n",
        "df_with_revenue.select(\"fare_amount\", \"tip_amount\", \"total_amount\", \"Revenue\").show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Do-ENQhcgTjk",
        "outputId": "493a6623-b2e2-487d-efd9-a46acecf3f95"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+------------+-------+\n",
            "|fare_amount|tip_amount|total_amount|Revenue|\n",
            "+-----------+----------+------------+-------+\n",
            "|        6.0|      1.47|       11.27|  22.54|\n",
            "|        7.0|       1.5|        12.3|   24.6|\n",
            "|        6.0|       1.0|        10.8|   21.6|\n",
            "|        5.5|      1.36|        8.16|  16.32|\n",
            "|        3.5|       0.0|         4.8|    9.6|\n",
            "+-----------+----------+------------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_revenue.groupBy(\"PULocationID\") \\\n",
        "    .sum(\"passenger_count\") \\\n",
        "    .orderBy(\"sum(passenger_count)\", ascending=False) \\\n",
        "    .show(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2TklA-0gnh4",
        "outputId": "34cc087f-0524-4212-c4ff-79fdcac1a944"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+--------------------+\n",
            "|PULocationID|sum(passenger_count)|\n",
            "+------------+--------------------+\n",
            "|         237|              433243|\n",
            "|         161|              425986|\n",
            "|         236|              403347|\n",
            "|         230|              360096|\n",
            "|         162|              351011|\n",
            "|         186|              338952|\n",
            "|         132|              326402|\n",
            "|          48|              297148|\n",
            "|         142|              294502|\n",
            "|         170|              289593|\n",
            "+------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_revenue.groupBy(\"VendorID\") \\\n",
        "    .avg(\"total_amount\") \\\n",
        "    .withColumnRenamed(\"avg(total_amount)\", \"avg_total_earning\") \\\n",
        "    .orderBy(\"VendorID\") \\\n",
        "    .show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eyYPR8mi3s5",
        "outputId": "c494824d-b812-4cb3-baeb-d7fdfb5f1ad0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------------------+\n",
            "|VendorID| avg_total_earning|\n",
            "+--------+------------------+\n",
            "|    null|37.217091425863046|\n",
            "|       1|18.113429405148675|\n",
            "|       2|18.648347164220418|\n",
            "+--------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, to_timestamp\n",
        "\n",
        "df_time = df_with_revenue.withColumn(\"pickup_time\", to_timestamp(\"tpep_pickup_datetime\"))\n"
      ],
      "metadata": {
        "id": "YxZol1nCjVAd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import window\n",
        "\n",
        "df_time.groupBy(\n",
        "    window(\"pickup_time\", \"1 hour\"), \"payment_type\"\n",
        ").count().orderBy(\"window\").show(10, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pUVWy1Ijc1M",
        "outputId": "e414fd1e-7186-488c-9509-62fda861b34e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------+------------+-----+\n",
            "|window                                    |payment_type|count|\n",
            "+------------------------------------------+------------+-----+\n",
            "|{2003-01-01 00:00:00, 2003-01-01 01:00:00}|2           |1    |\n",
            "|{2008-12-31 23:00:00, 2009-01-01 00:00:00}|1           |5    |\n",
            "|{2008-12-31 23:00:00, 2009-01-01 00:00:00}|2           |5    |\n",
            "|{2009-01-01 00:00:00, 2009-01-01 01:00:00}|2           |12   |\n",
            "|{2009-01-01 00:00:00, 2009-01-01 01:00:00}|1           |1    |\n",
            "|{2009-01-01 01:00:00, 2009-01-01 02:00:00}|2           |2    |\n",
            "|{2009-01-01 02:00:00, 2009-01-01 03:00:00}|2           |2    |\n",
            "|{2009-01-01 04:00:00, 2009-01-01 05:00:00}|2           |1    |\n",
            "|{2009-01-01 10:00:00, 2009-01-01 11:00:00}|2           |1    |\n",
            "|{2019-12-18 15:00:00, 2019-12-18 16:00:00}|1           |2    |\n",
            "+------------------------------------------+------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "df_date = df_with_revenue.withColumn(\"trip_date\", to_date(\"tpep_pickup_datetime\"))\n"
      ],
      "metadata": {
        "id": "ujnFeqJQjfYo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum as _sum\n",
        "\n",
        "top_vendors = df_date.groupBy(\"VendorID\", \"trip_date\") \\\n",
        "    .agg(\n",
        "        _sum(\"total_amount\").alias(\"total_earning\"),\n",
        "        _sum(\"passenger_count\").alias(\"total_passengers\"),\n",
        "        _sum(\"trip_distance\").alias(\"total_distance\")\n",
        "    ) \\\n",
        "    .orderBy(\"trip_date\", _sum(\"total_amount\").desc())\n",
        "\n",
        "top_vendors.show(5, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOOGwPLojk36",
        "outputId": "8b3c0230-1e1f-48e5-cc81-28d2e6b334e4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+------------------+----------------+------------------+\n",
            "|VendorID|trip_date |total_earning     |total_passengers|total_distance    |\n",
            "+--------+----------+------------------+----------------+------------------+\n",
            "|2       |2003-01-01|0.0               |1               |0.0               |\n",
            "|2       |2008-12-31|268.75            |17              |62.83999999999999 |\n",
            "|2       |2009-01-01|441.20000000000005|30              |93.00999999999999 |\n",
            "|2       |2019-12-18|9.11              |5               |0.0               |\n",
            "|2       |2019-12-31|2323.0799999999986|287             |414.07000000000016|\n",
            "+--------+----------+------------------+----------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum as _sum\n",
        "\n",
        "df_with_revenue.groupBy(\"PULocationID\", \"DOLocationID\") \\\n",
        "    .agg(_sum(\"passenger_count\").alias(\"total_passengers\")) \\\n",
        "    .orderBy(\"total_passengers\", ascending=False) \\\n",
        "    .show(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgYbfk4Tjm8T",
        "outputId": "8ba94382-5e16-4bcf-edee-4d8cd54dd4c5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------+----------------+\n",
            "|PULocationID|DOLocationID|total_passengers|\n",
            "+------------+------------+----------------+\n",
            "|         237|         236|           67885|\n",
            "|         236|         236|           57662|\n",
            "|         236|         237|           56488|\n",
            "|         237|         237|           49757|\n",
            "|         264|         264|           44789|\n",
            "|         239|         238|           30402|\n",
            "|         239|         142|           28755|\n",
            "|         161|         237|           27492|\n",
            "|         142|         239|           27260|\n",
            "|         186|         230|           25857|\n",
            "+------------+------------+----------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max\n",
        "\n",
        "latest_time = df_with_revenue.select(max(\"tpep_pickup_datetime\")).collect()[0][0]\n",
        "print(\"Latest pickup time:\", latest_time)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elTnQgScju-V",
        "outputId": "4207491c-2fe9-4e24-d35b-bd299a9dea75"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest pickup time: 2021-01-02 01:12:10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_timestamp, col\n",
        "from datetime import timedelta\n",
        "\n",
        "df_recent = df_with_revenue.withColumn(\"pickup_time\", to_timestamp(\"tpep_pickup_datetime\"))\n",
        "\n",
        "# Calculate the timestamp 10 seconds before the latest time\n",
        "time_threshold = latest_time - timedelta(seconds=10)\n",
        "\n",
        "df_last10s = df_recent.filter(\n",
        "    col(\"pickup_time\") >= time_threshold\n",
        ")\n",
        "\n",
        "df_last10s.groupBy(\"PULocationID\") \\\n",
        "    .agg({\"passenger_count\": \"sum\"}) \\\n",
        "    .orderBy(\"sum(passenger_count)\", ascending=False) \\\n",
        "    .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAkdvLDej7YR",
        "outputId": "a060d1de-4cc3-458b-d08c-e7c528b010a3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+--------------------+\n",
            "|PULocationID|sum(passenger_count)|\n",
            "+------------+--------------------+\n",
            "|          90|                   1|\n",
            "+------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}